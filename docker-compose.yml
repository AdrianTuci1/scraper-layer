version: '3.8'

services:
  # Node.js API Service
  scraper-node:
    build:
      context: ./scraper_node
      dockerfile: Dockerfile
      target: production
    container_name: scraper-node-api
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - PORT=3000
      - AWS_REGION=${AWS_REGION}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - API_KEY=${API_KEY}
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - RATE_LIMIT_MAX=${RATE_LIMIT_MAX:-100}
      - ALLOWED_ORIGINS=${ALLOWED_ORIGINS}
      - DYNAMODB_JOBS_TABLE=${DYNAMODB_JOBS_TABLE}
      - DYNAMODB_PIPELINES_TABLE=${DYNAMODB_PIPELINES_TABLE}
      - SQS_QUEUE_URL=${SQS_QUEUE_URL}
      - S3_BUCKET=${S3_BUCKET}
      - STAGE=${STAGE:-prod}
      - REGION=${AWS_REGION}
    env_file:
      - .env
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3000/health', (res) => { process.exit(res.statusCode === 200 ? 0 : 1) })"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - scraper-network

  # Go Scraper Engine Service
  scraper-go:
    build:
      context: ./scraper_go
      dockerfile: Dockerfile
      target: production
    container_name: scraper-go-engine
    environment:
      - AWS_REGION=${AWS_REGION}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - SQS_QUEUE_URL=${SQS_QUEUE_URL}
      - S3_BUCKET_NAME=${S3_BUCKET_NAME}
      - NODE_API_URL=${NODE_API_URL}
      - API_KEY=${API_KEY}
      - MAX_CONCURRENT_JOBS=${MAX_CONCURRENT_JOBS:-100}
      - WORKER_POOL_SIZE=${WORKER_POOL_SIZE:-10}
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - LOG_FORMAT=${LOG_FORMAT:-json}
      - PROXY_LIST=${PROXY_LIST}
      - USE_PROXY_ROTATION=${USE_PROXY_ROTATION:-false}
      - MAX_RETRIES=${MAX_RETRIES:-3}
      - RETRY_DELAY=${RETRY_DELAY:-5s}
    env_file:
      - .env
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - scraper-network
    depends_on:
      - scraper-node

networks:
  scraper-network:
    driver: bridge

volumes:
  scraper-logs:
    driver: local
